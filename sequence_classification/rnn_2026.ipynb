{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Task\n",
    "\n",
    "In this challenge, we want to train a classifier for sequences of genetic code.\n",
    "\n",
    "Each sequence is represented by a string of letters [‘A’, ‘C’, ‘G’, ’T’] and belongs to one of five categories/classes labelled [0,…,4].\n",
    "\n",
    "For training purposes, you will find 400 labelled sequences, each of length 400 characters (sequences: data_x, labels: data_y).\n",
    "\n",
    "To validate your model, you have a further 100 labelled sequences (val_x, val_y) with 1200 characters each.\n",
    "\n",
    "Finally, you have 250 unlabeled sequences (test_x, 2000 characters) which need to be classified.\n",
    "\n",
    "Hint: Training recurrent networks is very expensive! Do not start working on this challenge too late or you will not manage to finish in time.\n",
    "\n",
    "Your task is to train an RNN-based classifier and make a prediction for the missing labels of the test set (test_x in the attached archive). Store your prediction as a one-dimensional numpy.ndarray, save this array as prediction.npy, and upload this file to the KVV.\n",
    "\n",
    "You will receive points according to the achieved accuracy according to the following table:\n",
    "accuracy \tpoints\n",
    "\n",
    "≥95%=10, ≥90%=7, ≥85%=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow-gpu as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Data Loading\n",
    "with np.load('rnn-challenge-data.npz') as fh:\n",
    "    x_train, y_train = fh['data_x'], fh['data_y']\n",
    "    x_val, y_val     = fh['val_x'], fh['val_y']\n",
    "    x_test           = fh['test_x']\n",
    "\n",
    "# 2. Vectorization (Optimized with np.eye)\n",
    "mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "def encode_sequences(data):\n",
    "    # Converts 'A' -> 0, 'C' -> 1, etc., then to one-hot\n",
    "    encoded = np.array([[mapping[char] for char in seq] for seq in data])\n",
    "    return tf.one_hot(encoded, depth=4).numpy()\n",
    "\n",
    "x_train_vec = encode_sequences(x_train)\n",
    "x_val_vec   = encode_sequences(x_val)\n",
    "x_test_vec  = encode_sequences(x_test)\n",
    "\n",
    "# Convert labels to categorical (One-Hot)\n",
    "y_train_ohe = to_categorical(y_train, num_classes=5)\n",
    "y_val_ohe   = to_categorical(y_val, num_classes=5)\n",
    "\n",
    "# 3. Model Architecture\n",
    "model = Sequential([\n",
    "    # Bidirectional LSTMs often capture genetic patterns better\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(None, 4)),\n",
    "    BatchNormalization(), # Stabilizes training and allows higher learning rates\n",
    "    Dropout(0.2),         # Prevents overfitting\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(5, activation='softmax') # Use softmax for multi-class classification\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 4. Training with EarlyStopping (Better than custom threshold)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=20, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_vec, y_train_ohe,\n",
    "    validation_data=(x_val_vec, y_val_ohe),\n",
    "    epochs=200, # Should converge much faster now\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 5. Prediction & Saving\n",
    "predictions = np.argmax(model.predict(x_test_vec), axis=1)\n",
    "np.save('prediction.npy', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT prediction FROM x_test\n",
    "import numpy as np\n",
    "predictions = model.predict(x_test_data_floats)\n",
    "predictions = np.argmax(predictions, axis=1) # THAT'S YOUR JOB\n",
    "print(predictions.shape)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT YOU HAVE THE RIGHT FORMAT\n",
    "assert predictions.ndim == 1\n",
    "assert predictions.shape[0] == 250\n",
    "\n",
    "# AND SAVE EXACTLY AS SHOWN BELOW\n",
    "np.save('results/prediction.npy', predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
